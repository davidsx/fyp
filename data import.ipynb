{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# from nba_utils import draw_3pt_piechart,plot_shot_chart\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.core.magic import register_cell_magic, register_line_cell_magic, register_line_magic\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "from matplotlib import colorbar as cbar\n",
    "from matplotlib import cbook as cbook\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import array, col, count, mean, sum, udf, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, Row\n",
    "from pyspark.sql.functions import sum, col, udf\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set_style(\"white\")\n",
    "# sns.set_color_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = StructType([ \n",
    "    StructField(\"IncidntNum\", StringType(), True), \n",
    "    StructField(\"Category\", StringType(), True), \n",
    "    StructField(\"Description\", StringType(), True), \n",
    "    StructField(\"DayOfWeek\", StringType(), True), \n",
    "    StructField(\"Date_str\", StringType(), True), \n",
    "    StructField(\"Time_str\", StringType(), True), \n",
    "    StructField(\"District\", StringType(), True), \n",
    "    StructField(\"Resolution\", StringType(), True), \n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"PdId\", StringType(), True), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFsource = spark.read.csv('s3a://davidsx/dataSFcrime/', header=True, schema = customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, concat_ws\n",
    "\n",
    "crimeDF = crimeDFsource.withColumn(\"DateTime_str\", concat_ws(' ',crimeDFsource.Date_str,crimeDFsource.Time_str))\\\n",
    ".withColumn('Datetime',to_timestamp('DateTime_str', 'MM/dd/yyyy HH:mm').cast(TimestampType()))\\\n",
    "\n",
    "crimeDF = crimeDF.withColumn(\"Year\", year(\"Datetime\")).withColumn(\"Month\", month(\"Datetime\"))\\\n",
    ".withColumn(\"DayOfMonth\",dayofmonth(\"Datetime\")).withColumn(\"Hour\", hour(\"Datetime\"))\\\n",
    ".withColumn(\"Minute\", minute(\"Datetime\"))\n",
    "\n",
    "crimeDF = crimeDF.drop('Date_str','Time_str','DateTime_str','Resolution','Location','PdId')\n",
    "crimeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDF.cache()\n",
    "sqlContext.registerDataFrameAsTable(crimeDF, 'df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
